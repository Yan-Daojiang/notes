# 数据存储与检索
数据库在插入数据时，就保存数据，而在查询时，就检索数据。数据是如何存储，查询时，又是如何进行检索的呢？这就涉及到数据库内部的实现了。尽管现在不用从头开发一套新的存储引擎，但是了解相关的知识能够让我们针对特定的场景选择合适的存储引擎，针对一些数据库进行调优时，也需要对底层的存储引擎有一个基本了解。不同的场景和需求下，存储引擎也会做不同的优化，例如针对事务型负载（OLTP）和分析型（OLAP）负载的存储引擎就存在巨大差异。在 OLTP 方面，存储引擎又可以分为两大家族：面向日志结构的存储引擎和面向页的存储引擎。

## 数据库的核心：数据结构
最简单的数据库原理如下，类似日志（log），每次插入数据时就追加到文件的尾部，而查询数据时，则在文件中倒查。
```Shell
#!/bin/bash

db_set () {
    echo "$1,$2" >> database
}

db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```
然而，这个数据库存在很多问题，其中最常见的就是其查询数据的性能太低，它的查找开销是 O(n)。

为了提高查询的性能，通常会新的数据结构：索引。索引是通过保留一些额外的元数据，并通过它们来定位需要查找的数据，根据查找的方式不同可以建立不同的索引。尽管索引的引入能够提高查询的性能，然而，索引的维护是需要额外的开销的，特别是在新数据写入的时候，需要更新索引，因而任何类型的索引通常都会降低写入的速度。因此，在设计的时候也就需要就此进行权衡，适当的索引能够加速读取的速度，但是同时又会降低写入速度。

### 哈希索引
最容易想到的是键-值（key-value, kv）数据的索引，因为它和编程语言中的字典结构类似，可以很容易的将它扩展到磁盘上索引数据。

最简单的策略就是：保存内存中的 hash map，可以将每个键映射为文件特定字节的偏移量（文件中的存储位置），通过这样的方式能够提供高性能的读写，只需磁盘寻址一次，如果操作系统已经缓存了那部分文件数据，在读取的时候甚至可以避免磁盘I/O。这种模式下存储引擎，非常适合一定规模的键存储需求，同时值需要频繁更新的场景。

上面这种模式在写数据的时候，如果只是不断追加一个文件那么该如何避免磁盘空间用尽呢？一种好的解决方式就是日志分解为一定大小的段，每个段使用自己的内存哈希表。当段文件达到一定的大小的时候就关闭，后续的写入使用新的文件，已经关闭的文件段可以进行压缩合并。该过程可以使用后台线程去完成，在运行的时候，旧的段文件可以继续处理读写请求，合并完成之后，切换到新的文件，并可以将旧的文件安全删除。

从理论上看哈希索引似乎非常简单，然而，在真；正的实现中还需要考虑下面一些重要问题：
* 文件格式：应当使用合适的文件格式来存储值；
* 记录删除：删除某个键关联的值的时候需要在数据库文件中设置特殊的删除记录（称为墓碑）。合并的过程中，一旦碰到墓碑，就应该丢弃这个已删键的所有值；
* 崩溃恢复：数据库重启，内存中的哈希表将丢失，理论上可以通过文件重建哈希表，但是在段文件很大的时候可能需要很长的时间恢复。通过快照等方式能够加快恢复的速度；
* 部分写入问题：数据库系统可能在任何时候崩溃，因此可能造成部分写入问题，常见的解决方式又写入校验值等；
* 并发控制：写入需要以严格的顺序写入到日志文件中，通常仅设置一个写入线程，并且文件是只追加的，这样可以被多个线程同时读取。

哈希索引看起来很好，但是也有其天然的局限性：
* 哈希索引需要所有的键被保存在内存中，当键的数量巨大时，将很难进行处理。目前，在磁盘上很难维护哈希表，主要是其需要大量的随机I/O。
* 哈希索引并不适合区间查询。
为了突破这些限制，需要一些其他的索引结构。


### SSTbles 和 LSM-Tree
在前面的哈希索引的日志存储段中，k-v的存储顺序并不重要，仅仅按照写入顺序进行存储。现在改变段文件的格式，要求段文件按照 k-v 对按照键的顺序进行排列，并且每个键在段中只能够出现一次，这种结构就称之为：排序字符串表（SSTables）。与上面哈希索引使用的普通的追加日志段相比，这具有如下的有点：
* 段的合并变得简单，合并的过程类似归并排序的过程，多个段包含相同的键时，使用最新的键。
* 查找特定的键的值以及对应的值时，并不需要在内存中保存所有的键。因为段文件中的键是有序的，例如要找 handiwork 时，内存中没有这个键，但是有 handbag 和 handsome，根据排序的特点就可以从 handbag 开始进行顺序查找。因此，此时虽然仍然需要在内存中构建哈希表，但是它可以是稀疏的。
* 读请通常需要扫描请求范围内的多个k-v对，因此可以考虑将这些记录保存在一个块中，并在写磁盘之前进行压缩，并用内存中的索引条目指向压缩的块头。这样既可以节省磁盘空间，又可以减少I/O带宽占用。

#### SSTables 的构建与维护
SSTables 看起来有很多具有很多的优势，其关键是段文件按键排序。写入通常是按照任意顺序出现的，如何实现按序排列呢？此时就是一些树状数据结构（红黑树/AVL树）发挥作用的时刻了。使用这些数据结构能够高效的维护按序插入，其存储引擎的工作流程大致如下：

写入时：
* 写入内存中的树状数据结构，这个树也称为内存表；
* 当内存表的大小达到一定的阈值时，将其作为 SSTables 段文件写入磁盘；
* 写磁盘的过程中的新的写入可以写到内存表。

读取时：
* 首先查找内存中的内存表，如果不存在，按照新旧顺序一次查找磁盘中的 SSTables 文件。

段压缩合并：
* 后台线程周期性的执行段合并和压缩。

为了防止崩溃时，内存中的内存表中的数据丢失，还需要在磁盘上保存单独的日志文件，每次写入都追加到这个文件中，这样崩溃后可以用它来恢复内存表。

#### SSTables 到 LSM-Tree
LevelDB 和 RocksDB 所用使用的一些算法的本质就是 SSTables。类似的存储引擎还被用于 Cassandra和HBase，它们都受到 BigTables 论文的影响。这种索引结构最初以日志合并树（LSM-Tree）命名，其建立在更早期的日志文件系统之上。因此，基于合并和压缩排序文件的存储引擎通常也被称为 LSM 存储引擎。

在Lucenne等全文搜索系统中使用的索引引擎也采用了一些类似的方法。

#### 性能优化点
实际在应用上面的一些理论的时候，还需要考虑一些优化的点，例如查找一个不存在的键可能很慢，为了优化这种访问，存储引擎通常会使用额外的布隆过滤器。

不同的压缩策略和时机也会对性能有影响，最常见的是大小分级和分层压缩。

由于数据是按照顺序存储的，因此也可以高效的执行区间查询，并且由于磁盘是顺序写入的，因此，LSM-Tree 可支持高吞吐量的写入。

### B-trees
B-trees 是最常见的索引结构，几乎是所有关系型数据库的标准索引实现，在许多非关系型数据库中也经常使用。

与日志索引结构将数据库分解为不同大小的段不同，B-tree将数据库分解为固定大小的块或页（传统大小为4KB），页是内部读/写的最小单元。这种设计接近底层结构，因为磁盘也是按固定大小的块排列。页面之间通过磁盘地址相互引用，可以构造出一个树状结构。某一页会被指定为B-tree的根，查找时总是从该页开始，该也包含若干个键和对子页的引用。每个孩子负责连续范围的键，相邻引用之间的键可指示范围的边界。B-tree中一个页包含的子页的引用数量称为分支因子。实际中分支因子取决于存储页面引用和范围边界所需的空间总量，通常为几百个。

更新B-tree的键对应的值时，首先查找包含该键的叶子页，更新并写回磁盘。如果是新增，找到其范围包含的新键的页，并添加；如果没有足够的空间，那么将其分解为两个半满的页，并更新父页键范围。

B-tree保持树平衡，N个键的B-tree具有O（logN）的深度。大多数的数据库可以适合3～4层的B-tree（分子因子为500的4KB页四级树可以存储高达256TB)。

#### B-tree 可靠
B-tree底层的写操作是用新的数据覆盖磁盘上的就数据，不会改变磁盘上的存储位置。但是某些操作会覆盖不同的页，例如页溢出需要写两个分裂的页，并且需要更新父页面的引用。这个操作是很危险的，如果部分写入的时候发生了崩溃，将会导致索引被破坏。

为了让数据库崩溃之后能够恢复，B-tree在实现的时候也需要额外的数据结构——预写日志（write-ahead log, WAL，也称为重做日志）。B-tree 的修改必须先追加WAL，再修改树本身的结构。发生崩溃的时候使用该日志进行恢复。

此外，多个线程并发访问的时候还需要做并发控制，否则线程看到的树的结构可能处于不一致的状态。通常使用轻量级的锁保护相应的数据结构。

#### 优化点
* 不使用覆盖页和维护WAL进行崩溃恢复，而是采用写时复制技术，该方式也有助于并发控制。
* 压缩键来节省页空间，提高分支因子，从而减小树的层高。
* 页在磁盘上进行布局优化，尽量进行顺序I/O。
* 添加额外的指针，如兄弟页面之间的指针。
* B-tree变体，如分形树。
